############################################
#### 13/10
############################################
- implémentation lecture d'instance.
- choix basique de la matrice d'adjacence pour représenter le graphe.
- validité vérifiée par le print du nombre d'edges versus la somme des 1 de la matrice
- average time of 0.0215 on 300 loads of brock200_2.col
- tests on scipy sparse matrix structures:
    The size of the sparse structure CSR seems interesting as compared to base adjacency matrix on bigger
    graphs (the size of CSR being discussed as the size of the class as well as the three main sub_arrays indices, indptr and
    data that are underlying) :
    Nom du graphe  Taille de CSR  Taille de adjacency
    brock200_2.col 1188 40128
    dsjc125.1.col 888 15753
    random-10.col 428 228
    random-100.col 788 10128
    random-40.col 548 1728
    random-70.col 668 5028

- Then made sparse construction directly from instance file rather than adjacency
-> Need to compare time for creations

############################################
#### 14/10
############################################
- compared time of sparse versus adjacency
- should we store everything as booleans ?
- building neighbourhoods is doable, but will eventually explode in terms
of complexity
- for n=200, building V_3 already takes 5E-2s, for 19900 neighbours
Possibles heuristics:
    - choose starting nodes (biggest degrees ?) and grow from it a maximal clique (not maximum) -> linear
    - il y a des algos en m^(3/2) pour les triangles
    - https://en.wikipedia.org/wiki/Clique_problem#Finding_a_single_maximal_clique : trouver un unique triangle ou pas d'existence
    peut se faire en encore mieux
- ajout calcul du degré dans la construction des données
- Idée d'heuristique perso : ranger les sommets par deg décroissant et tenter de construire une clique comme ça.

############################################
#### 15/10
############################################
- What we are looking for is a sub matrix with only ones except for the diagonal that is made of 0, is it usable ?
- Done with most basic heuristic
    Amelioration : If at some point the remaining degrees are less than the number
    of elements in the clique, no more node can be added. Very slight time gain.
- Done a dynamic version of basic heuristic, strangely it is faster and less
 efficient ??
 - added randomized versions where at each step we take one of the k possible candidates. For now k is arbitrarely set at deg_max // 2
 What value of k should be taken ?

############################################
#### 16/10
############################################
- Found interesting paper : https://www.researchgate.net/publication/341273157_Randomized_heuristic_for_the_maximum_clique_problem
The idea of descending degrees seems to be valid and can be worked on.
- On other instances, we see that, quite naturally, adding some randomness can yield better solutions, but of course not always.

############################################
#### 17/10
############################################
- worked on performance of random base heuristic

############################################
#### 18/10
############################################
- the problem we have here is that our neighbours will have either
0 or 1 as a delta for the objective, which makes it hard to discriminate
between them.
- première méta : faire un vns de base où on retire k noeuds de la clique et on refait une descente à partir de là.
- voisinage V_i(x) = retirer i noeuds de la clique x
- base idea : remove nodes and add others back with biggest degrees first. -> doesn't work
because we are always rebuilding the same clique.
- Works with random candidate taken
- To be done :
     display base results and final one found with vns
     display common nodes % with base solution

############################################
#### 19/10
############################################
- using np random shuffle gains a lot of time in random function instead of for loop.
- augmenting max neighbourhood size only seems to push the last best iteration further without actually 
helping to find a best solution
- we converge quickly in a local optima from which we have difficulties to get out of. As such we need to 
add randomness here and maybe restart from the least seen nodes.
- rewrote vns base as runner
- seems like the one restarting from other positions is not so good.
- enhanced definitively doesn't seem to help
- Enhancement of adding node to clique test.
# Mean with adjacency comparison: Mean 0.024424775600433348
# Mean with bit sum is less efficient : Mean 0.03597150468826294
# Mean with np.take and sum is better: Mean 0.006640812635421753

# Same on biggest graph for nodes : "C2000.5.txt"
# Mean with adjacency comparison: Mean 0.05538655281066895
# Mean with np.take and sum is better: Mean 0.017236299753189088
# Mean with naive loove even with possibly getting out faster : Mean 0.8596100306510925

# the factor 4 difference seems to stay true


############################################
#### 20/10
############################################
- the issue of early convergence might come from the fact that without improvement we stay at best clique
an idea might be to reverse this and instead keep a best, modify a current and regularly get back to best if we
don't find an improvement
- exploration can yield better results as well as worse ones, need to improve
- the best with exploration can be better but doesn't seem to be easily linked to the number of
    exploring iterations.
- exploring now takes the best neighbourh instead of going through the first
one
-  adding the possibility to take a solution of same value increases the chance to cycle
 -> we need taboos
 - taboos provide solution more different and even slightly better it seems !
 
 - biggest neighbourhood is kept fixed at size 5
 - max_best_iterations_without_improve is taken so that the probability
 of having not removed a node from the clique for V_1 of the first clique
 found with the heuristic is less than 1/2
 Seems less effective -> to be benchmarked.
